{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 1: Filtrar eventos lunares y solares de los últimos dos años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"data/eventos\")\n",
    "\n",
    "# Filtrar eventos\n",
    "eventos_filtrados = df \\\n",
    "    .withColumn(\"year\", year(\"timestamp\")) \\\n",
    "    .filter(\n",
    "        (col(\"event_type\").isin([\"lunar\", \"solar\"])) & \n",
    "        (col(\"year\") >= 2023)\n",
    "    )\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Eventos lunares y solares desde 2023:\")\n",
    "eventos_filtrados.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 2: Agrupar eventos por tipo y calcular la media de eventos por año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_eventos = df \\\n",
    "    .withColumn(\"year\", year(\"timestamp\")) \\\n",
    "    .groupBy(\"event_type\", \"year\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_eventos\")\n",
    "    ) \\\n",
    "    .groupBy(\"event_type\") \\\n",
    "    .agg(\n",
    "        avg(\"total_eventos\").alias(\"media_eventos_por_año\")\n",
    "    )\n",
    "\n",
    "print(\"Media de eventos por tipo:\")\n",
    "media_eventos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 3: Implementar broadcast join con dataset de ubicaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ubicaciones_data = [\n",
    "    (\"NorteAmerica\", \"GMT-5\", \"Norte\"),\n",
    "    (\"SurAmerica\", \"GMT-3\", \"Sur\"),\n",
    "    (\"Europa\", \"GMT+1\", \"Norte\"),\n",
    "    (\"Asia\", \"GMT+8\", \"Norte\"),\n",
    "    (\"Africa\", \"GMT+2\", \"Sur\"),\n",
    "    (\"Oceania\", \"GMT+10\", \"Sur\")\n",
    "]\n",
    "\n",
    "ubicaciones_df = spark.createDataFrame(\n",
    "    ubicaciones_data, \n",
    "    [\"location\", \"zona_horaria\", \"hemisferio\"]\n",
    ")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "eventos_con_ubicacion = df \\\n",
    "    .join(broadcast(ubicaciones_df), \"location\") \\\n",
    "    .select(\"event_id\", \"event_type\", \"location\", \"zona_horaria\", \"hemisferio\")\n",
    "\n",
    "print(\"Eventos con información de ubicación:\")\n",
    "eventos_con_ubicacion.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 4: Escribir datos procesados en Parquet con particionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df \\\n",
    "    .withColumn(\"year\", year(\"timestamp\")) \\\n",
    "    .withColumn(\"month\", month(\"timestamp\")) \\\n",
    "    .write \\\n",
    "    .partitionBy(\"year\", \"location\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"data/eventos_particionados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 5: Leer CSV con datos corruptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrupto = spark.read \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"data/eventos_corruptos.csv\")\n",
    "\n",
    "print(\"Datos leídos ignorando filas corruptas:\")\n",
    "df_corrupto.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 6: Calcular duración promedio por tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventos_duracion = df \\\n",
    "    .withColumn(\"duracion_minutos\", \n",
    "        unix_timestamp(\"timestamp\") - unix_timestamp(lag(\"timestamp\")\n",
    "        .over(Window.partitionBy(\"event_type\").orderBy(\"timestamp\")))\n",
    "    ) \\\n",
    "    .groupBy(\"event_type\") \\\n",
    "    .agg(\n",
    "        avg(\"duracion_minutos\").alias(\"duracion_promedio_minutos\")\n",
    "    )\n",
    "\n",
    "print(\"Duración promedio por tipo de evento:\")\n",
    "eventos_duracion.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 7: Pipeline incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'pipeline_eventos_incrementales',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline incremental para procesar eventos',\n",
    "    schedule_interval='0 * * * *',  # Cada hora\n",
    ")\n",
    "\n",
    "procesar_task = PythonOperator(\n",
    "    task_id='procesar_datos_incrementales',\n",
    "    python_callable=main,\n",
    "    dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 8: Configuración para memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.executor.memory\", \"8g\")\n",
    "spark.conf.set(\"spark.driver.memory\", \"4g\")\n",
    "spark.conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.memory.offHeap.size\", \"8g\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 9: Cache vs Persist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cache\n",
    "df.cache()\n",
    "cached_time = time.time()\n",
    "df.groupBy(\"event_type\").count().show()\n",
    "print(f\"Tiempo con cache: {time.time() - cached_time}\")\n",
    "\n",
    "# Persist\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "df.unpersist()\n",
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "persist_time = time.time()\n",
    "df.groupBy(\"event_type\").count().show()\n",
    "print(f\"Tiempo con persist: {time.time() - persist_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 10: Optimizar con particionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_optimizado = df \\\n",
    "    .repartition(col(\"event_type\")) \\\n",
    "    .sortWithinPartitions(\"timestamp\")\n",
    "\n",
    "# Guardar optimizado\n",
    "df_optimizado.write \\\n",
    "    .partitionBy(\"event_type\") \\\n",
    "    .option(\"maxRecordsPerFile\", 1000000) \\\n",
    "    .parquet(\"data/eventos_optimizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sql "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 1: Consulta de eventos lunares por ubicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT location, COUNT(*) AS event_count\n",
    "FROM fact_events\n",
    "WHERE event_type = 'lunar'\n",
    "GROUP BY location;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 2: Análisis de esquema estrella"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "fact_events (event_id, event_type_id, timestamp_id, location_id, details)\n",
    "dim_event_type (event_type_id, event_name)\n",
    "dim_timestamp (timestamp_id, year, month, day)\n",
    "dim_location (location_id, location_name, coordinates)\n",
    "\n",
    "Mejoras propuestas:\n",
    "\n",
    "Agregar índices en claves foráneas\n",
    "Particionar por año\n",
    "Agregar columnas de auditoría"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 3: Rango de eventos entre años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    event_type,\n",
    "    MIN(year) as primer_año,\n",
    "    MAX(year) as ultimo_año,\n",
    "    COUNT(*) as total_eventos\n",
    "FROM fact_events fe\n",
    "JOIN dim_timestamp dt ON fe.timestamp_id = dt.timestamp_id\n",
    "GROUP BY event_type;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 4: Índices para dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE INDEX idx_event_type_id ON fact_events(event_type_id);\n",
    "CREATE INDEX idx_timestamp_id ON fact_events(timestamp_id);\n",
    "CREATE INDEX idx_location_id ON fact_events(location_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 5: Tabla de auditoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE audit_fact_events (\n",
    "    audit_id SERIAL PRIMARY KEY,\n",
    "    event_id VARCHAR(36),\n",
    "    action_type VARCHAR(10),\n",
    "    changed_at TIMESTAMP,\n",
    "    changed_by VARCHAR(50)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 6: Promedios por año y ubicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    l.location_name,\n",
    "    t.year,\n",
    "    COUNT(*) as total_eventos,\n",
    "    AVG(COUNT(*)) OVER (PARTITION BY l.location_name) as promedio_ubicacion\n",
    "FROM fact_events fe\n",
    "JOIN dim_location l ON fe.location_id = l.location_id\n",
    "JOIN dim_timestamp t ON fe.timestamp_id = t.timestamp_id\n",
    "GROUP BY l.location_name, t.year;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 7: Esquema Snowflake para tiempo y ubicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE dim_region (\n",
    "    region_id INT PRIMARY KEY,\n",
    "    region_name VARCHAR(50)\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_location (\n",
    "    location_id INT PRIMARY KEY,\n",
    "    location_name VARCHAR(50),\n",
    "    region_id INT REFERENCES dim_region(region_id)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 8: Picos de eventos lunares por trimestre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WITH eventos_trimestre AS (\n",
    "    SELECT \n",
    "        t.year,\n",
    "        t.quarter,\n",
    "        COUNT(*) as eventos,\n",
    "        AVG(COUNT(*)) OVER () as promedio_general\n",
    "    FROM fact_events fe\n",
    "    JOIN dim_timestamp t ON fe.timestamp_id = t.timestamp_id\n",
    "    WHERE event_type = 'lunar'\n",
    "    GROUP BY t.year, t.quarter\n",
    ")\n",
    "SELECT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 9: Optimizar consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Antes\n",
    "SELECT *\n",
    "FROM fact_events\n",
    "WHERE event_type = 'lunar';\n",
    "\n",
    "-- Después (con índice y particionamiento)\n",
    "CREATE INDEX idx_event_type ON fact_events(event_type)\n",
    "INCLUDE (event_id, location_id, timestamp_id);\n",
    "\n",
    "SELECT /*+ INDEX(fact_events idx_event_type) */\n",
    "    fe.event_id, \n",
    "    l.location_name,\n",
    "    t.event_date\n",
    "FROM fact_events fe\n",
    "JOIN dim_location l ON fe.location_id = l.location_id\n",
    "JOIN dim_timestamp t ON fe.timestamp_id = t.timestamp_id\n",
    "WHERE fe.event_type = 'lunar';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 10: Estrategia para datos históricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Particionamiento por rango de fechas\n",
    "CREATE TABLE fact_events_history (\n",
    "    event_id VARCHAR(36),\n",
    "    event_type_id INT,\n",
    "    timestamp_id INT,\n",
    "    location_id INT\n",
    ") PARTITION BY RANGE (timestamp_id);\n",
    "\n",
    "-- Crear particiones por año\n",
    "CREATE TABLE fact_events_2020 PARTITION OF fact_events_history\n",
    "    FOR VALUES FROM (20200101) TO (20210101);\n",
    "CREATE TABLE fact_events_2021 PARTITION OF fact_events_history\n",
    "    FOR VALUES FROM (20210101) TO (20220101);"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
